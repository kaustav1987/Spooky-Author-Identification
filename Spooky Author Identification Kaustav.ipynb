{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spooky Author Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from sklearn.svm import SVC\n",
    "from keras.layers import  Conv1D,GlobalMaxPooling1D, MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "##from keras.models.pooling  import \n",
    "from keras.layers import Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train   = pd.read_csv('train.csv')\n",
    "df_test    = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       EAP       HPL       MWS\n",
       "0  id02310  0.403494  0.287808  0.308698\n",
       "1  id24541  0.403494  0.287808  0.308698\n",
       "2  id00134  0.403494  0.287808  0.308698\n",
       "3  id27757  0.403494  0.287808  0.308698\n",
       "4  id04081  0.403494  0.287808  0.308698"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps) ## so that we never take log of zero\n",
    "    n = actual.shape[0] ##no of examples \n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / n * vsota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert text labels to integers, 0, 1 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(df_train['author'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Data is not balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7900,)\n",
      "(5635,)\n",
      "(6044,)\n"
     ]
    }
   ],
   "source": [
    "print(y[y==0].shape)\n",
    "print(y[y==1].shape)\n",
    "print(y[y==2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed =1987\n",
    "X_train, X_valid, y_train,y_valid = train_test_split(df_train['text'], y,\n",
    "                                                   stratify = y, test_size = 0.1,shuffle= True,\n",
    "                                                   random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7110,)\n",
      "(5071,)\n",
      "(5440,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train[y_train==0].shape)\n",
    "print(y_train[y_train==1].shape)\n",
    "print(y_train[y_train==2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17621,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(min_df = 5, max_features = None, strip_accents = 'unicode', analyzer = 'word', \n",
    "                      token_pattern= r'\\w{1,}', ngram_range= (1,3), use_idf = True, smooth_idf = True, \n",
    "                     sublinear_tf = True , stop_words = 'english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv.fit(X_train)\n",
    "X_train_tfv = tfv.transform(X_train)\n",
    "X_valid_tfv = tfv.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF Loss 0.6298162023590189\n"
     ]
    }
   ],
   "source": [
    "## Logistic on tfidf\n",
    "clf = LogisticRegression(C= 1.0)\n",
    "clf.fit(X_train_tfv, y_train)\n",
    "predictions = clf.predict_proba(X_valid_tfv)\n",
    "\n",
    "print('TFIDF Loss', multiclass_logloss(y_valid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Pred 0  Pred 1  Pred 2\n",
      "True 0     655      70      65\n",
      "True 1     101     423      40\n",
      "True 2     109      33     462\n"
     ]
    }
   ],
   "source": [
    "prediction_class = clf.predict(X_valid_tfv)\n",
    "\n",
    "confusion = confusion_matrix(y_valid,prediction_class,labels = [0,1,2])\n",
    "\n",
    "confusion = pd.DataFrame(confusion, index=['True 0', 'True 1','True 2'],\n",
    "                         columns=['Pred 0','Pred 1', 'Pred 2'])\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic on countvectorizer\n",
    "ctv = CountVectorizer(analyzer = 'word',token_pattern= r'\\w{1,}',ngram_range = (1,3), stop_words = 'english' )\n",
    "ctv.fit(X_train)\n",
    "X_train_ctv = ctv.transform(X_train)\n",
    "X_valid_ctv = ctv.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer Loss 0.5362277258308014\n",
      "\n",
      "\n",
      "        Pred 0  Pred 1  Pred 2\n",
      "True 0     678      54      58\n",
      "True 1     118     401      45\n",
      "True 2     119      28     457\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C= 1.0)\n",
    "clf.fit(X_train_ctv, y_train)\n",
    "predictions = clf.predict_proba(X_valid_ctv)\n",
    "\n",
    "print('Count Vectorizer Loss', multiclass_logloss(y_valid, predictions))\n",
    "print('\\n')\n",
    "####\n",
    "prediction_class = clf.predict(X_valid_ctv)\n",
    "\n",
    "confusion = confusion_matrix(y_valid,prediction_class,labels = [0,1,2])\n",
    "\n",
    "confusion = pd.DataFrame(confusion, index=['True 0', 'True 1','True 2'],\n",
    "                         columns=['Pred 0','Pred 1', 'Pred 2'])\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.590 \n"
     ]
    }
   ],
   "source": [
    "# simple Naive Bayes on TFIDF\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_tfv, y_train)\n",
    "predictions = clf.predict_proba(X_valid_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.468 \n"
     ]
    }
   ],
   "source": [
    "# simple Naive Bayes on CountVectorizer\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_ctv, y_train)\n",
    "predictions = clf.predict_proba(X_valid_ctv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.745 \n"
     ]
    }
   ],
   "source": [
    "## Reduce the number of features of TDIDF using SVD before fitting to SVM\n",
    "\n",
    "## Reduce the number of component chosen is 150 (120-200 range)\n",
    "\n",
    "svd = decomposition.TruncatedSVD(n_components = 150)\n",
    "svd.fit(X_train_tfv)\n",
    "X_train_svd = svd.transform(X_train_tfv)\n",
    "X_valid_svd = svd.transform(X_valid_tfv)\n",
    "\n",
    "####\n",
    "# Scale the data obtained from SVD.\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(X_train_svd)\n",
    "X_train_svd_scl = scl.transform(X_train_svd)\n",
    "X_valid_svd_scl = scl.transform(X_valid_svd)\n",
    "####\n",
    "## simple SVM\n",
    "clf = SVC(C =1.0, probability = True)\n",
    "clf.fit(X_train_svd_scl, y_train)\n",
    "predictions = clf.predict_proba(X_valid_svd_scl)\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.984 \n"
     ]
    }
   ],
   "source": [
    "## Using the TDIDF itself to reduce the number of features\n",
    "## This performed worse\n",
    "\n",
    "tfv1 = TfidfVectorizer(min_df = 3, max_features = 150, strip_accents = 'unicode', analyzer = 'word', \n",
    "                      token_pattern= r'\\w{1,}', ngram_range= (1,3), use_idf = True, smooth_idf = True, \n",
    "                     sublinear_tf = True , stop_words = 'english')\n",
    "\n",
    "tfv1.fit(X_train)\n",
    "X_train_tfv1 = tfv1.transform(X_train)\n",
    "X_valid_tfv1 = tfv1.transform(X_valid)\n",
    "\n",
    "## simple SVM\n",
    "clf = SVC(C =1.0, probability = True)\n",
    "clf.fit(X_train_tfv1, y_train)\n",
    "predictions = clf.predict_proba(X_valid_tfv1)\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.777 \n"
     ]
    }
   ],
   "source": [
    "## simple xgboost on tf-idf\n",
    "\n",
    "clf = xgb.XGBClassifier(max_depth = 7, n_estimators = 200, colsample_bytree =0.8,\n",
    "                       subsample = 0.8, nthread = 10, learning_rate = 0.1)\n",
    "\n",
    "clf.fit(X_train_tfv.tocsc(),y_train) ##Convert this matrix to Compressed Sparse Column format.\n",
    "predictions = clf.predict_proba(X_valid_tfv.tocsc())\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.773 \n"
     ]
    }
   ],
   "source": [
    "## simple xgboost on countvectorizer\n",
    "\n",
    "clf = xgb.XGBClassifier(max_depth = 7, n_estimators = 200, colsample_bytree =0.8,\n",
    "                       subsample = 0.8, nthread = 10, learning_rate = 0.1)\n",
    "\n",
    "clf.fit(X_train_ctv.tocsc(),y_train) ##Convert this matrix to Compressed Sparse Column format.\n",
    "predictions = clf.predict_proba(X_valid_ctv.tocsc())\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.831 \n"
     ]
    }
   ],
   "source": [
    "##xgboost on tf-idf svd features\n",
    "\n",
    "clf = xgb.XGBClassifier(thread = 10)\n",
    "\n",
    "clf.fit(X_train_svd,y_train) ##Convert this matrix to Compressed Sparse Column format.\n",
    "predictions = clf.predict_proba(X_valid_svd)\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer_func = metrics.make_scorer(multiclass_logloss, greater_is_better = False, needs_proba = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "\n",
    "svd = TruncatedSVD()\n",
    "##\n",
    "scl = preprocessing.StandardScaler()\n",
    "##\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "clf = pipeline.Pipeline([('svd', svd), ('scl', scl), ('lr', logistic_model)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'svd__n_components': [120,150,180],\n",
    "              'lr__C': [0.1,1.0,10],\n",
    "              'lr__penalty': ['l1','l2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -0.739\n",
      "Best parameters set: {'memory': None, 'steps': [('svd', TruncatedSVD(algorithm='randomized', n_components=180, n_iter=5,\n",
      "       random_state=None, tol=0.0)), ('scl', StandardScaler(copy=True, with_mean=True, with_std=True)), ('lr', LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False))], 'svd': TruncatedSVD(algorithm='randomized', n_components=180, n_iter=5,\n",
      "       random_state=None, tol=0.0), 'scl': StandardScaler(copy=True, with_mean=True, with_std=True), 'lr': LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), 'svd__algorithm': 'randomized', 'svd__n_components': 180, 'svd__n_iter': 5, 'svd__random_state': None, 'svd__tol': 0.0, 'scl__copy': True, 'scl__with_mean': True, 'scl__with_std': True, 'lr__C': 0.1, 'lr__class_weight': None, 'lr__dual': False, 'lr__fit_intercept': True, 'lr__intercept_scaling': 1, 'lr__max_iter': 100, 'lr__multi_class': 'warn', 'lr__n_jobs': None, 'lr__penalty': 'l2', 'lr__random_state': None, 'lr__solver': 'warn', 'lr__tol': 0.0001, 'lr__verbose': 0, 'lr__warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "##Grid Search Model\n",
    "\n",
    "model = GridSearchCV(estimator = clf, param_grid=param_grid, scoring = scorer_func, n_jobs = -1, \n",
    "                     iid= True, refit = True,cv =2)\n",
    "\n",
    "model.fit(X_train_tfv, y_train)\n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "print(\"Best parameters set:\", best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tlr__C: 0.1\n",
      "\tlr__penalty: 'l2'\n",
      "\tsvd__n_components: 180\n"
     ]
    }
   ],
   "source": [
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -0.505\n",
      "Best parameters set:\n",
      "\tnb__alpha: 0.1\n"
     ]
    }
   ],
   "source": [
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Create the pipeline \n",
    "clf = pipeline.Pipeline([('nb', nb_model)])\n",
    "\n",
    "# parameter grid\n",
    "param_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Initialize Grid Search Model\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=scorer_func,\n",
    "                                 n_jobs=-1, iid=True, refit=True, cv=2)\n",
    "\n",
    "# Fit Grid Search Model\n",
    "model.fit(X_train_tfv, y_train)  # we can use the full data here but im only using xtrain. \n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -0.501\n",
      "Best parameters set:\n",
      "\tnb__alpha: 1\n"
     ]
    }
   ],
   "source": [
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Create the pipeline \n",
    "clf = pipeline.Pipeline([('nb', nb_model)])\n",
    "\n",
    "# parameter grid\n",
    "param_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Initialize Grid Search Model\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=scorer_func,\n",
    "                                 n_jobs=-1, iid=True, refit=True, cv=2)\n",
    "\n",
    "# Fit Grid Search Model \n",
    "model.fit(X_train_ctv, y_train)  # we can use the full data here but im only using xtrain. \n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "### More Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -0.493\n",
      "Best parameters set:\n",
      "\tnb__alpha: 1.7\n"
     ]
    }
   ],
   "source": [
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Create the pipeline \n",
    "clf = pipeline.Pipeline([('nb', nb_model)])\n",
    "\n",
    "# parameter grid\n",
    "param_grid = {'nb__alpha': [0.8, 1,1.5,1.6,1.7,1.75,1.8,1.9,2,2.5]}  \n",
    "\n",
    "# Initialize Grid Search Model\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=scorer_func,\n",
    "                                 n_jobs=-1, iid=True, refit=True, cv=2)\n",
    "\n",
    "# Fit Grid Search Model \n",
    "model.fit(X_train_ctv, y_train)  # we can use the full data here but im only using xtrain. \n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2196017"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = len(open('glove.840B.300d.txt', encoding='utf-8').readlines(  ))\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isfloat(value):\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd32bedcb2b4a29bebefa49b6d61f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2196017), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exceptional case: . . .\n",
      "Exceptional case: at name@domain.com\n",
      "Exceptional case: . . . . .\n",
      "Exceptional case: to name@domain.com\n",
      "Exceptional case: . .\n",
      "Exceptional case: . . . .\n",
      "Exceptional case: email name@domain.com\n",
      "Exceptional case: or name@domain.com\n",
      "Exceptional case: contact name@domain.com\n",
      "Exceptional case: Email name@domain.com\n",
      "Exceptional case: on name@domain.com\n",
      "Exceptional case: At Killerseats.com\n",
      "Exceptional case: by name@domain.com\n",
      "Exceptional case: in mylot.com\n",
      "Exceptional case: emailing name@domain.com\n",
      "Exceptional case: Contact name@domain.com\n",
      "Exceptional case: at name@domain.com\n",
      "Exceptional case: • name@domain.com\n",
      "Exceptional case: at Amazon.com\n",
      "Exceptional case: is name@domain.com\n",
      "2195903 word vectors found in globe\n"
     ]
    }
   ],
   "source": [
    "## # load the GloVe 300D vectors in a dictionary:\n",
    "\n",
    "embedding_index = {}\n",
    "\n",
    "with open('glove.840B.300d.txt', encoding='utf-8') as f:\n",
    "    for line in tqdm_notebook(f,total=2196017):\n",
    "        value = line.split()\n",
    "        word = value[0]\n",
    "        ##print(value)\n",
    "        i = 1\n",
    "        while not isfloat(value[i]):\n",
    "            word = word + ' ' + value[i]\n",
    "            i = i + 1\n",
    "        if i > 1:\n",
    "            print('Exceptional case: {0}'.format(word))\n",
    "        coefs = np.asarray(value[i:], dtype='float32')\n",
    "        embedding_index[word] = coef\n",
    "        ##break\n",
    "print(len(embedding_index), 'word vectors found in globe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creates a normalized vector for the whole sentence\n",
    "\n",
    "def sent2vec(s):\n",
    "    ##words = str(s).lower().decode('utf-8')\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    \n",
    "    vector = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            vector.append(embedding_index[word])\n",
    "        except:\n",
    "            continue\n",
    "    vector = np.array(vector)\n",
    "    \n",
    "    v = vector.sum(axis =0)## reduce the dimension  ## may use squeeze\n",
    "    ##print(type(v))\n",
    "    if type(v) != np.ndarray:\n",
    "        return  np.zeros(300)\n",
    "    return v/np.sqrt((v**2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28ed479c9024e23995180abf7500c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=17621), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de3abeb49254d96a5f9586aa7192c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1958), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create sentence vectors\n",
    "\n",
    "X_train_glove = [sent2vec(s) for s in tqdm_notebook(X_train, total= len(X_train))]\n",
    "X_valid_glove = [sent2vec(s) for s in tqdm_notebook(X_valid, total= len(X_valid))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove = np.array(X_train_glove)\n",
    "X_valid_glove = np.array(X_valid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.066 \n"
     ]
    }
   ],
   "source": [
    "##simple xgboost on glove features\n",
    "clf = xgb.XGBClassifier(nthread=10)\n",
    "clf.fit(X_train_glove, y_train)\n",
    "predictions = clf.predict_proba(X_valid_glove)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.069 \n"
     ]
    }
   ],
   "source": [
    "##simple xgboost on glove features\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "\n",
    "clf.fit(X_train_glove, y_train)\n",
    "predictions = clf.predict_proba(X_valid_glove)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LSTM and GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data before any neural net:\n",
    "\n",
    "scl = preprocessing.StandardScaler()\n",
    "X_train_glove_scl = scl.fit_transform(X_train_glove)\n",
    "X_valid_glove_scl = scl.transform(X_valid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to binarize the labels for the neural net\n",
    "y_train_enc = np_utils.to_categorical(y_train)\n",
    "y_valid_enc = np_utils.to_categorical(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 0 ... 2 2 0]\n",
      "\n",
      "\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)\n",
    "print('\\n')\n",
    "print(y_train_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple 3 layer sequential neural net\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(300, input_dim=300, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/5\n",
      " - 15s - loss: 1.2840 - val_loss: 1.1076\n",
      "Epoch 2/5\n",
      " - 5s - loss: 1.1456 - val_loss: 1.0887\n",
      "Epoch 3/5\n",
      " - 4s - loss: 1.1092 - val_loss: 1.0880\n",
      "Epoch 4/5\n",
      " - 4s - loss: 1.1010 - val_loss: 1.0898\n",
      "Epoch 5/5\n",
      " - 4s - loss: 1.1004 - val_loss: 1.0904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25faeb73c18>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_glove_scl, y=y_train_enc, batch_size=64, \n",
    "          epochs=5, verbose=2, \n",
    "          validation_data=(X_valid_glove_scl, y_valid_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "##using keras\n",
    "max_len = 70\n",
    "\n",
    "token = text.Tokenizer(num_words = None)\n",
    "token.fit_on_texts(X_train)\n",
    "X_train_seq = token.texts_to_sequences(X_train)\n",
    "X_valid_seq = token.texts_to_sequences(X_valid)\n",
    "##\n",
    "X_train_pad = sequence.pad_sequences(X_train_seq,maxlen= max_len)\n",
    "X_valid_pad = sequence.pad_sequences(X_valid_seq,maxlen= max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "\n",
    "for words, index in word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "##simple LSTM with glove embeddings and two dense layers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index)+1,300,weights=[embedding_matrix],\n",
    "                   input_length=max_len,trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/100\n",
      " - 6s - loss: 1.1099 - val_loss: 1.0901\n",
      "Epoch 2/100\n",
      " - 4s - loss: 1.0912 - val_loss: 1.0894\n",
      "Epoch 3/100\n",
      " - 4s - loss: 1.0900 - val_loss: 1.0880\n",
      "Epoch 4/100\n",
      " - 4s - loss: 1.0892 - val_loss: 1.0880\n",
      "Epoch 5/100\n",
      " - 5s - loss: 1.0900 - val_loss: 1.0879\n",
      "Epoch 6/100\n",
      " - 5s - loss: 1.0892 - val_loss: 1.0883\n",
      "Epoch 7/100\n",
      " - 5s - loss: 1.0886 - val_loss: 1.0878\n",
      "Epoch 8/100\n",
      " - 5s - loss: 1.0881 - val_loss: 1.0875\n",
      "Epoch 9/100\n",
      " - 4s - loss: 1.0887 - val_loss: 1.0877\n",
      "Epoch 10/100\n",
      " - 4s - loss: 1.0883 - val_loss: 1.0876\n",
      "Epoch 11/100\n",
      " - 5s - loss: 1.0889 - val_loss: 1.0879\n",
      "Epoch 12/100\n",
      " - 5s - loss: 1.0886 - val_loss: 1.0879\n",
      "Epoch 13/100\n",
      " - 4s - loss: 1.0884 - val_loss: 1.0876\n",
      "Epoch 14/100\n",
      " - 4s - loss: 1.0882 - val_loss: 1.0877\n",
      "Epoch 15/100\n",
      " - 4s - loss: 1.0882 - val_loss: 1.0878\n",
      "Epoch 16/100\n",
      " - 4s - loss: 1.0879 - val_loss: 1.0875\n",
      "Epoch 17/100\n",
      " - 4s - loss: 1.0882 - val_loss: 1.0876\n",
      "Epoch 18/100\n",
      " - 4s - loss: 1.0884 - val_loss: 1.0876\n",
      "Epoch 19/100\n",
      " - 4s - loss: 1.0881 - val_loss: 1.0876\n",
      "Epoch 20/100\n",
      " - 4s - loss: 1.0880 - val_loss: 1.0875\n",
      "Epoch 21/100\n",
      " - 4s - loss: 1.0880 - val_loss: 1.0877\n",
      "Epoch 22/100\n",
      " - 4s - loss: 1.0880 - val_loss: 1.0876\n",
      "Epoch 23/100\n",
      " - 4s - loss: 1.0880 - val_loss: 1.0876\n",
      "Epoch 24/100\n",
      " - 4s - loss: 1.0879 - val_loss: 1.0875\n",
      "Epoch 25/100\n",
      " - 5s - loss: 1.0879 - val_loss: 1.0875\n",
      "Epoch 26/100\n",
      " - 5s - loss: 1.0878 - val_loss: 1.0876\n",
      "Epoch 27/100\n",
      " - 5s - loss: 1.0879 - val_loss: 1.0876\n",
      "Epoch 28/100\n",
      " - 5s - loss: 1.0881 - val_loss: 1.0875\n",
      "Epoch 29/100\n",
      " - 5s - loss: 1.0882 - val_loss: 1.0877\n",
      "Epoch 30/100\n",
      " - 5s - loss: 1.0881 - val_loss: 1.0875\n",
      "Epoch 31/100\n",
      " - 5s - loss: 1.0881 - val_loss: 1.0875\n",
      "Epoch 32/100\n",
      " - 5s - loss: 1.0878 - val_loss: 1.0877\n",
      "Epoch 33/100\n",
      " - 5s - loss: 1.0882 - val_loss: 1.0876\n",
      "Epoch 34/100\n",
      " - 5s - loss: 1.0877 - val_loss: 1.0876\n",
      "Epoch 35/100\n",
      " - 5s - loss: 1.0879 - val_loss: 1.0876\n",
      "Epoch 36/100\n",
      " - 4s - loss: 1.0880 - val_loss: 1.0876\n",
      "Epoch 37/100\n",
      " - 5s - loss: 1.0877 - val_loss: 1.0875\n",
      "Epoch 38/100\n",
      " - 5s - loss: 1.0876 - val_loss: 1.0875\n",
      "Epoch 39/100\n",
      " - 5s - loss: 1.0877 - val_loss: 1.0875\n",
      "Epoch 40/100\n",
      " - 5s - loss: 1.0879 - val_loss: 1.0876\n",
      "Epoch 41/100\n",
      " - 5s - loss: 1.0879 - val_loss: 1.0876\n",
      "Epoch 42/100\n",
      " - 5s - loss: 1.0876 - val_loss: 1.0876\n",
      "Epoch 43/100\n",
      " - 5s - loss: 1.0879 - val_loss: 1.0876\n",
      "Epoch 44/100\n",
      " - 5s - loss: 1.0878 - val_loss: 1.0875\n",
      "Epoch 45/100\n",
      " - 5s - loss: 1.0879 - val_loss: 1.0876\n",
      "Epoch 46/100\n",
      " - 5s - loss: 1.0878 - val_loss: 1.0875\n",
      "Epoch 47/100\n",
      " - 5s - loss: 1.0880 - val_loss: 1.0875\n",
      "Epoch 48/100\n",
      " - 5s - loss: 1.0874 - val_loss: 1.0875\n",
      "Epoch 49/100\n",
      " - 5s - loss: 1.0880 - val_loss: 1.0875\n",
      "Epoch 50/100\n",
      " - 5s - loss: 1.0876 - val_loss: 1.0876\n",
      "Epoch 51/100\n",
      " - 5s - loss: 1.0878 - val_loss: 1.0875\n",
      "Epoch 52/100\n",
      " - 5s - loss: 1.0877 - val_loss: 1.0875\n",
      "Epoch 53/100\n",
      " - 5s - loss: 1.0876 - val_loss: 1.0875\n",
      "Epoch 54/100\n",
      " - 5s - loss: 1.0874 - val_loss: 1.0875\n",
      "Epoch 55/100\n",
      " - 5s - loss: 1.0874 - val_loss: 1.0875\n",
      "Epoch 56/100\n",
      " - 5s - loss: 1.0879 - val_loss: 1.0876\n",
      "Epoch 57/100\n",
      " - 5s - loss: 1.0877 - val_loss: 1.0875\n",
      "Epoch 58/100\n",
      " - 5s - loss: 1.0879 - val_loss: 1.0875\n",
      "Epoch 59/100\n",
      " - 5s - loss: 1.0876 - val_loss: 1.0875\n",
      "Epoch 60/100\n",
      " - 5s - loss: 1.0879 - val_loss: 1.0875\n",
      "Epoch 61/100\n",
      " - 5s - loss: 1.0877 - val_loss: 1.0875\n",
      "Epoch 62/100\n",
      " - 5s - loss: 1.0879 - val_loss: 1.0876\n",
      "Epoch 63/100\n",
      " - 5s - loss: 1.0876 - val_loss: 1.0875\n",
      "Epoch 64/100\n",
      " - 5s - loss: 1.0875 - val_loss: 1.0875\n",
      "Epoch 65/100\n",
      " - 5s - loss: 1.0876 - val_loss: 1.0875\n",
      "Epoch 66/100\n",
      " - 5s - loss: 1.0879 - val_loss: 1.0875\n",
      "Epoch 67/100\n",
      " - 5s - loss: 1.0878 - val_loss: 1.0875\n",
      "Epoch 68/100\n",
      " - 5s - loss: 1.0877 - val_loss: 1.0875\n",
      "Epoch 69/100\n",
      " - 5s - loss: 1.0876 - val_loss: 1.0875\n",
      "Epoch 70/100\n",
      " - 5s - loss: 1.0878 - val_loss: 1.0875\n",
      "Epoch 71/100\n",
      " - 5s - loss: 1.0877 - val_loss: 1.0875\n",
      "Epoch 72/100\n",
      " - 5s - loss: 1.0878 - val_loss: 1.0875\n",
      "Epoch 73/100\n",
      " - 5s - loss: 1.0875 - val_loss: 1.0875\n",
      "Epoch 74/100\n",
      " - 5s - loss: 1.0876 - val_loss: 1.0875\n",
      "Epoch 75/100\n",
      " - 5s - loss: 1.0876 - val_loss: 1.0875\n",
      "Epoch 76/100\n",
      " - 5s - loss: 1.0876 - val_loss: 1.0875\n",
      "Epoch 77/100\n",
      " - 5s - loss: 1.0876 - val_loss: 1.0875\n",
      "Epoch 78/100\n",
      " - 5s - loss: 1.0876 - val_loss: 1.0875\n",
      "Epoch 79/100\n",
      " - 5s - loss: 1.0877 - val_loss: 1.0875\n",
      "Epoch 80/100\n",
      " - 5s - loss: 1.0876 - val_loss: 1.0875\n",
      "Epoch 81/100\n",
      " - 5s - loss: 1.0876 - val_loss: 1.0875\n",
      "Epoch 82/100\n",
      " - 5s - loss: 1.0876 - val_loss: 1.0875\n",
      "Epoch 83/100\n",
      " - 5s - loss: 1.0878 - val_loss: 1.0875\n",
      "Epoch 84/100\n",
      " - 5s - loss: 1.0877 - val_loss: 1.0875\n",
      "Epoch 85/100\n",
      " - 5s - loss: 1.0876 - val_loss: 1.0875\n",
      "Epoch 86/100\n",
      " - 5s - loss: 1.0874 - val_loss: 1.0875\n",
      "Epoch 87/100\n",
      " - 5s - loss: 1.0876 - val_loss: 1.0875\n",
      "Epoch 88/100\n",
      " - 5s - loss: 1.0875 - val_loss: 1.0875\n",
      "Epoch 89/100\n",
      " - 5s - loss: 1.0875 - val_loss: 1.0875\n",
      "Epoch 90/100\n",
      " - 5s - loss: 1.0877 - val_loss: 1.0875\n",
      "Epoch 91/100\n",
      " - 5s - loss: 1.0877 - val_loss: 1.0874\n",
      "Epoch 92/100\n",
      " - 5s - loss: 1.0874 - val_loss: 1.0867\n",
      "Epoch 93/100\n",
      " - 5s - loss: 1.0852 - val_loss: 1.0814\n",
      "Epoch 94/100\n",
      " - 5s - loss: 1.0810 - val_loss: 1.0792\n",
      "Epoch 95/100\n",
      " - 5s - loss: 1.0809 - val_loss: 1.0753\n",
      "Epoch 96/100\n",
      " - 5s - loss: 1.0786 - val_loss: 1.0827\n",
      "Epoch 97/100\n",
      " - 5s - loss: 1.0798 - val_loss: 1.0767\n",
      "Epoch 98/100\n",
      " - 5s - loss: 1.0784 - val_loss: 1.0801\n",
      "Epoch 99/100\n",
      " - 5s - loss: 1.0786 - val_loss: 1.0747\n",
      "Epoch 100/100\n",
      " - 5s - loss: 1.0781 - val_loss: 1.0764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25faeb61550>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_pad, y=y_train_enc, batch_size=512, epochs=100, verbose=2, validation_data=(X_valid_pad, y_valid_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/100\n",
      " - 5s - loss: 1.0760 - val_loss: 1.0767\n",
      "Epoch 2/100\n",
      " - 5s - loss: 1.0773 - val_loss: 1.0750\n",
      "Epoch 3/100\n",
      " - 5s - loss: 1.0758 - val_loss: 1.0732\n",
      "Epoch 4/100\n",
      " - 4s - loss: 1.0769 - val_loss: 1.0752\n",
      "Epoch 5/100\n",
      " - 4s - loss: 1.0760 - val_loss: 1.0751\n",
      "Epoch 6/100\n",
      " - 5s - loss: 1.0757 - val_loss: 1.0737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25fc0728668>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "\n",
    "model.fit(X_train_pad, y=y_train_enc, batch_size=512, epochs=100, \n",
    "          verbose=2, validation_data=(X_valid_pad, y_valid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/100\n",
      " - 15s - loss: 1.1183 - val_loss: 1.0892\n",
      "Epoch 2/100\n",
      " - 12s - loss: 1.0947 - val_loss: 1.0868\n",
      "Epoch 3/100\n",
      " - 12s - loss: 1.0910 - val_loss: 1.0862\n",
      "Epoch 4/100\n",
      " - 12s - loss: 1.0907 - val_loss: 1.0883\n",
      "Epoch 5/100\n",
      " - 12s - loss: 1.0893 - val_loss: 1.0868\n",
      "Epoch 6/100\n",
      " - 12s - loss: 1.0894 - val_loss: 1.0869\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26112a92dd8>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple bidirectional LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=2, mode='auto')\n",
    "model.fit(X_train_pad, y=y_train_enc, batch_size=512, epochs=100, \n",
    "          verbose=2, validation_data=(X_valid_pad, y_valid_enc), callbacks=[earlystop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/100\n",
      " - 15s - loss: 1.1621 - val_loss: 1.0924\n",
      "Epoch 2/100\n",
      " - 12s - loss: 1.1069 - val_loss: 1.0907\n",
      "Epoch 3/100\n",
      " - 12s - loss: 1.1018 - val_loss: 1.0902\n",
      "Epoch 4/100\n",
      " - 12s - loss: 1.0944 - val_loss: 1.0900\n",
      "Epoch 5/100\n",
      " - 12s - loss: 1.0924 - val_loss: 1.0900\n",
      "Epoch 6/100\n",
      " - 12s - loss: 1.0916 - val_loss: 1.0885\n",
      "Epoch 7/100\n",
      " - 12s - loss: 1.0916 - val_loss: 1.0888\n",
      "Epoch 8/100\n",
      " - 12s - loss: 1.0907 - val_loss: 1.0885\n",
      "Epoch 9/100\n",
      " - 12s - loss: 1.0894 - val_loss: 1.0878\n",
      "Epoch 10/100\n",
      " - 12s - loss: 1.0884 - val_loss: 1.0876\n",
      "Epoch 11/100\n",
      " - 12s - loss: 1.0887 - val_loss: 1.0876\n",
      "Epoch 12/100\n",
      " - 12s - loss: 1.0880 - val_loss: 1.0876\n",
      "Epoch 13/100\n",
      " - 12s - loss: 1.0881 - val_loss: 1.0875\n",
      "Epoch 14/100\n",
      " - 12s - loss: 1.0881 - val_loss: 1.0875\n",
      "Epoch 15/100\n",
      " - 12s - loss: 1.0878 - val_loss: 1.0876\n",
      "Epoch 16/100\n",
      " - 12s - loss: 1.0881 - val_loss: 1.0876\n",
      "Epoch 00016: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x261187f1c18>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GRU with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=2, mode='auto')\n",
    "model.fit(X_train_pad, y=y_train_enc, batch_size=512, epochs=100, \n",
    "          verbose=2, validation_data=(X_valid_pad, y_valid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:50:26] INFO Found 3 classes\n",
      "[23:50:26] INFO Training Level 0 Fold # 1. Model # 0\n",
      "[23:50:26] INFO Predicting Level 0. Fold # 1. Model # 0\n",
      "[23:50:26] INFO Level 0. Fold # 1. Model # 0. Validation Score = 0.663318\n",
      "[23:50:26] INFO Training Level 0 Fold # 2. Model # 0\n",
      "[23:50:26] INFO Predicting Level 0. Fold # 2. Model # 0\n",
      "[23:50:26] INFO Level 0. Fold # 2. Model # 0. Validation Score = 0.659648\n",
      "[23:50:26] INFO Training Level 0 Fold # 3. Model # 0\n",
      "[23:50:26] INFO Predicting Level 0. Fold # 3. Model # 0\n",
      "[23:50:26] INFO Level 0. Fold # 3. Model # 0. Validation Score = 0.661545\n",
      "[23:50:26] INFO Level 0. Model # 0. Mean Score = 0.661503. Std Dev = 0.001498\n",
      "[23:50:26] INFO Training Level 0 Fold # 1. Model # 1\n",
      "[23:50:27] INFO Predicting Level 0. Fold # 1. Model # 1\n",
      "[23:50:27] INFO Level 0. Fold # 1. Model # 1. Validation Score = 0.569423\n",
      "[23:50:27] INFO Training Level 0 Fold # 2. Model # 1\n",
      "[23:50:28] INFO Predicting Level 0. Fold # 2. Model # 1\n",
      "[23:50:28] INFO Level 0. Fold # 2. Model # 1. Validation Score = 0.566786\n",
      "[23:50:28] INFO Training Level 0 Fold # 3. Model # 1\n",
      "[23:50:29] INFO Predicting Level 0. Fold # 3. Model # 1\n",
      "[23:50:29] INFO Level 0. Fold # 3. Model # 1. Validation Score = 0.563724\n",
      "[23:50:29] INFO Level 0. Model # 1. Mean Score = 0.566644. Std Dev = 0.002329\n",
      "[23:50:30] INFO Training Level 0 Fold # 1. Model # 2\n",
      "[23:50:30] INFO Predicting Level 0. Fold # 1. Model # 2\n",
      "[23:50:30] INFO Level 0. Fold # 1. Model # 2. Validation Score = 0.487078\n",
      "[23:50:30] INFO Training Level 0 Fold # 2. Model # 2\n",
      "[23:50:30] INFO Predicting Level 0. Fold # 2. Model # 2\n",
      "[23:50:30] INFO Level 0. Fold # 2. Model # 2. Validation Score = 0.477949\n",
      "[23:50:30] INFO Training Level 0 Fold # 3. Model # 2\n",
      "[23:50:30] INFO Predicting Level 0. Fold # 3. Model # 2\n",
      "[23:50:30] INFO Level 0. Fold # 3. Model # 2. Validation Score = 0.482063\n",
      "[23:50:30] INFO Level 0. Model # 2. Mean Score = 0.482363. Std Dev = 0.003733\n",
      "[23:50:30] INFO Training Level 0 Fold # 1. Model # 3\n",
      "[23:50:30] INFO Predicting Level 0. Fold # 1. Model # 3\n",
      "[23:50:30] INFO Level 0. Fold # 1. Model # 3. Validation Score = 0.477828\n",
      "[23:50:30] INFO Training Level 0 Fold # 2. Model # 3\n",
      "[23:50:30] INFO Predicting Level 0. Fold # 2. Model # 3\n",
      "[23:50:30] INFO Level 0. Fold # 2. Model # 3. Validation Score = 0.472782\n",
      "[23:50:30] INFO Training Level 0 Fold # 3. Model # 3\n",
      "[23:50:30] INFO Predicting Level 0. Fold # 3. Model # 3\n",
      "[23:50:30] INFO Level 0. Fold # 3. Model # 3. Validation Score = 0.470239\n",
      "[23:50:30] INFO Level 0. Model # 3. Mean Score = 0.473616. Std Dev = 0.003154\n",
      "[23:50:30] INFO Saving predictions for level # 0\n",
      "[23:50:30] INFO Training Level 1 Fold # 1. Model # 0\n",
      "[23:50:35] INFO Predicting Level 1. Fold # 1. Model # 0\n",
      "[23:50:35] INFO Level 1. Fold # 1. Model # 0. Validation Score = 0.447066\n",
      "[23:50:35] INFO Training Level 1 Fold # 2. Model # 0\n",
      "[23:50:40] INFO Predicting Level 1. Fold # 2. Model # 0\n",
      "[23:50:40] INFO Level 1. Fold # 2. Model # 0. Validation Score = 0.440104\n",
      "[23:50:40] INFO Training Level 1 Fold # 3. Model # 0\n",
      "[23:50:45] INFO Predicting Level 1. Fold # 3. Model # 0\n",
      "[23:50:45] INFO Level 1. Fold # 3. Model # 0. Validation Score = 0.431901\n",
      "[23:50:45] INFO Level 1. Model # 0. Mean Score = 0.439690. Std Dev = 0.006198\n",
      "[23:50:46] INFO Saving predictions for level # 1\n",
      "[23:50:46] INFO Training Fulldata Level 0. Model # 0\n",
      "[23:50:46] INFO Predicting Test Level 0. Model # 0\n",
      "[23:50:46] INFO Training Fulldata Level 0. Model # 1\n",
      "[23:50:47] INFO Predicting Test Level 0. Model # 1\n",
      "[23:50:47] INFO Training Fulldata Level 0. Model # 2\n",
      "[23:50:47] INFO Predicting Test Level 0. Model # 2\n",
      "[23:50:47] INFO Training Fulldata Level 0. Model # 3\n",
      "[23:50:47] INFO Predicting Test Level 0. Model # 3\n",
      "[23:50:47] INFO Training Fulldata Level 1. Model # 0\n",
      "[23:50:55] INFO Predicting Test Level 1. Model # 0\n"
     ]
    }
   ],
   "source": [
    "# this is the main ensembling class. \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"[%(asctime)s] %(levelname)s %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\", stream=sys.stdout)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Ensembler(object):\n",
    "    def __init__(self, model_dict, num_folds=3, task_type='classification', optimize=roc_auc_score,\n",
    "                 lower_is_better=False, save_path=None):\n",
    "        \"\"\"\n",
    "        Ensembler init function\n",
    "        :param model_dict: model dictionary, see README for its format\n",
    "        :param num_folds: the number of folds for ensembling\n",
    "        :param task_type: classification or regression\n",
    "        :param optimize: the function to optimize for, e.g. AUC, logloss, etc. Must have two arguments y_test and y_pred\n",
    "        :param lower_is_better: is lower value of optimization function better or higher\n",
    "        :param save_path: path to which model pickles will be dumped to along with generated predictions, or None\n",
    "        \"\"\"\n",
    "\n",
    "        self.model_dict = model_dict\n",
    "        self.levels = len(self.model_dict)\n",
    "        self.num_folds = num_folds\n",
    "        self.task_type = task_type\n",
    "        self.optimize = optimize\n",
    "        self.lower_is_better = lower_is_better\n",
    "        self.save_path = save_path\n",
    "\n",
    "        self.training_data = None\n",
    "        self.test_data = None\n",
    "        self.y = None\n",
    "        self.lbl_enc = None\n",
    "        self.y_enc = None\n",
    "        self.train_prediction_dict = None\n",
    "        self.test_prediction_dict = None\n",
    "        self.num_classes = None\n",
    "\n",
    "    def fit(self, training_data, y, lentrain):\n",
    "        \"\"\"\n",
    "        :param training_data: training data in tabular format\n",
    "        :param y: binary, multi-class or regression\n",
    "        :return: chain of models to be used in prediction\n",
    "        \"\"\"\n",
    "\n",
    "        self.training_data = training_data\n",
    "        self.y = y\n",
    "\n",
    "        if self.task_type == 'classification':\n",
    "            self.num_classes = len(np.unique(self.y))\n",
    "            logger.info(\"Found %d classes\", self.num_classes)\n",
    "            self.lbl_enc = LabelEncoder()\n",
    "            self.y_enc = self.lbl_enc.fit_transform(self.y)\n",
    "            kf = StratifiedKFold(n_splits=self.num_folds)\n",
    "            train_prediction_shape = (lentrain, self.num_classes)\n",
    "        else:\n",
    "            self.num_classes = -1\n",
    "            self.y_enc = self.y\n",
    "            kf = KFold(n_splits=self.num_folds)\n",
    "            train_prediction_shape = (lentrain, 1)\n",
    "\n",
    "        self.train_prediction_dict = {}\n",
    "        for level in range(self.levels):\n",
    "            self.train_prediction_dict[level] = np.zeros((train_prediction_shape[0],\n",
    "                                                          train_prediction_shape[1] * len(self.model_dict[level])))\n",
    "\n",
    "        for level in range(self.levels):\n",
    "\n",
    "            if level == 0:\n",
    "                temp_train = self.training_data\n",
    "            else:\n",
    "                temp_train = self.train_prediction_dict[level - 1]\n",
    "\n",
    "            for model_num, model in enumerate(self.model_dict[level]):\n",
    "                validation_scores = []\n",
    "                foldnum = 1\n",
    "                for train_index, valid_index in kf.split(self.train_prediction_dict[0], self.y_enc):\n",
    "                    logger.info(\"Training Level %d Fold # %d. Model # %d\", level, foldnum, model_num)\n",
    "\n",
    "                    if level != 0:\n",
    "                        l_training_data = temp_train[train_index]\n",
    "                        l_validation_data = temp_train[valid_index]\n",
    "                        model.fit(l_training_data, self.y_enc[train_index])\n",
    "                    else:\n",
    "                        l0_training_data = temp_train[0][model_num]\n",
    "                        if type(l0_training_data) == list:\n",
    "                            l_training_data = [x[train_index] for x in l0_training_data]\n",
    "                            l_validation_data = [x[valid_index] for x in l0_training_data]\n",
    "                        else:\n",
    "                            l_training_data = l0_training_data[train_index]\n",
    "                            l_validation_data = l0_training_data[valid_index]\n",
    "                        model.fit(l_training_data, self.y_enc[train_index])\n",
    "\n",
    "                    logger.info(\"Predicting Level %d. Fold # %d. Model # %d\", level, foldnum, model_num)\n",
    "\n",
    "                    if self.task_type == 'classification':\n",
    "                        temp_train_predictions = model.predict_proba(l_validation_data)\n",
    "                        self.train_prediction_dict[level][valid_index,\n",
    "                        (model_num * self.num_classes):(model_num * self.num_classes) +\n",
    "                                                       self.num_classes] = temp_train_predictions\n",
    "\n",
    "                    else:\n",
    "                        temp_train_predictions = model.predict(l_validation_data)\n",
    "                        self.train_prediction_dict[level][valid_index, model_num] = temp_train_predictions\n",
    "                    validation_score = self.optimize(self.y_enc[valid_index], temp_train_predictions)\n",
    "                    validation_scores.append(validation_score)\n",
    "                    logger.info(\"Level %d. Fold # %d. Model # %d. Validation Score = %f\", level, foldnum, model_num,\n",
    "                                validation_score)\n",
    "                    foldnum += 1\n",
    "                avg_score = np.mean(validation_scores)\n",
    "                std_score = np.std(validation_scores)\n",
    "                logger.info(\"Level %d. Model # %d. Mean Score = %f. Std Dev = %f\", level, model_num,\n",
    "                            avg_score, std_score)\n",
    "\n",
    "            logger.info(\"Saving predictions for level # %d\", level)\n",
    "            train_predictions_df = pd.DataFrame(self.train_prediction_dict[level])\n",
    "            train_predictions_df.to_csv(os.path.join(self.save_path, \"train_predictions_level_\" + str(level) + \".csv\"),\n",
    "                                        index=False, header=None)\n",
    "\n",
    "        return self.train_prediction_dict\n",
    "\n",
    "    def predict(self, test_data, lentest):\n",
    "        self.test_data = test_data\n",
    "        if self.task_type == 'classification':\n",
    "            test_prediction_shape = (lentest, self.num_classes)\n",
    "        else:\n",
    "            test_prediction_shape = (lentest, 1)\n",
    "\n",
    "        self.test_prediction_dict = {}\n",
    "        for level in range(self.levels):\n",
    "            self.test_prediction_dict[level] = np.zeros((test_prediction_shape[0],\n",
    "                                                         test_prediction_shape[1] * len(self.model_dict[level])))\n",
    "        self.test_data = test_data\n",
    "        for level in range(self.levels):\n",
    "            if level == 0:\n",
    "                temp_train = self.training_data\n",
    "                temp_test = self.test_data\n",
    "            else:\n",
    "                temp_train = self.train_prediction_dict[level - 1]\n",
    "                temp_test = self.test_prediction_dict[level - 1]\n",
    "\n",
    "            for model_num, model in enumerate(self.model_dict[level]):\n",
    "\n",
    "                logger.info(\"Training Fulldata Level %d. Model # %d\", level, model_num)\n",
    "                if level == 0:\n",
    "                    model.fit(temp_train[0][model_num], self.y_enc)\n",
    "                else:\n",
    "                    model.fit(temp_train, self.y_enc)\n",
    "\n",
    "                logger.info(\"Predicting Test Level %d. Model # %d\", level, model_num)\n",
    "\n",
    "                if self.task_type == 'classification':\n",
    "                    if level == 0:\n",
    "                        temp_test_predictions = model.predict_proba(temp_test[0][model_num])\n",
    "                    else:\n",
    "                        temp_test_predictions = model.predict_proba(temp_test)\n",
    "                    self.test_prediction_dict[level][:, (model_num * self.num_classes): (model_num * self.num_classes) +\n",
    "                                                                                        self.num_classes] = temp_test_predictions\n",
    "\n",
    "                else:\n",
    "                    if level == 0:\n",
    "                        temp_test_predictions = model.predict(temp_test[0][model_num])\n",
    "                    else:\n",
    "                        temp_test_predictions = model.predict(temp_test)\n",
    "                    self.test_prediction_dict[level][:, model_num] = temp_test_predictions\n",
    "\n",
    "            test_predictions_df = pd.DataFrame(self.test_prediction_dict[level])\n",
    "            test_predictions_df.to_csv(os.path.join(self.save_path, \"test_predictions_level_\" + str(level) + \".csv\"),\n",
    "                                       index=False, header=None)\n",
    "\n",
    "        return self.test_prediction_dict\n",
    "# specify the data to be used for every level of ensembling:\n",
    "train_data_dict = {0: [X_train_tfv, X_train_ctv, X_train_tfv, X_train_ctv], 1: [X_train_glove]}\n",
    "test_data_dict = {0: [X_valid_tfv, X_valid_ctv, X_valid_tfv, X_valid_ctv], 1: [X_valid_glove]}\n",
    "\n",
    "model_dict = {0: [LogisticRegression(), LogisticRegression(), MultinomialNB(alpha=0.1), MultinomialNB()],\n",
    "\n",
    "              1: [xgb.XGBClassifier(silent=True, n_estimators=120, max_depth=7)]}\n",
    "\n",
    "#####################\n",
    "ens = Ensembler(model_dict=model_dict, num_folds=3, task_type='classification',\n",
    "                optimize=multiclass_logloss, lower_is_better=True, save_path='')\n",
    "\n",
    "ens.fit(train_data_dict, y_train, lentrain=X_train_glove.shape[0])\n",
    "preds = ens.predict(test_data_dict, lentest=X_valid_glove.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42454302013495454"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for the valid error:\n",
    "multiclass_logloss(y_valid, preds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### There are lot of stuff I have not done here that can improve the performance. \n",
    "### Will add them later"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
